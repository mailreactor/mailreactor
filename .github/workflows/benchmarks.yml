name: Performance Benchmarks

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main]
  workflow_dispatch:  # Allow manual trigger

jobs:
  benchmark:
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'
      
      - name: Install uv
        uses: astral-sh/setup-uv@v5
        with:
          version: "latest"
      
      - name: Create virtual environment
        run: uv venv
      
      - name: Install dependencies
        run: |
          uv pip install -e ".[dev]"
      
      - name: Run performance benchmarks
        run: |
          uv run pytest tests/performance/ --benchmark-only --benchmark-json=benchmark-results.json
      
      - name: Store benchmark results
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results
          path: benchmark-results.json
      
      - name: Check performance thresholds
        run: |
          echo "Performance benchmark results:"
          cat benchmark-results.json | python -m json.tool || true
          
          # Future: Add threshold checking here
          # For now, just display results
          echo ""
          echo "âœ… Performance benchmarks completed"
          echo "âš ï¸  Automated threshold checking will be added after baseline is established"
      
      - name: Comment benchmark results on PR
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const benchmark = JSON.parse(fs.readFileSync('benchmark-results.json', 'utf8'));
            
            let comment = '## ðŸ“Š Performance Benchmark Results\n\n';
            comment += '| Test | Min | Max | Mean | Median |\n';
            comment += '|------|-----|-----|------|--------|\n';
            
            benchmark.benchmarks.forEach(b => {
              const stats = b.stats;
              comment += `| ${b.name} | ${(stats.min*1000).toFixed(2)}ms | ${(stats.max*1000).toFixed(2)}ms | ${(stats.mean*1000).toFixed(2)}ms | ${(stats.median*1000).toFixed(2)}ms |\n`;
            });
            
            comment += '\n*Benchmarks run on Python 3.10 (ubuntu-latest)*';
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.name,
              body: comment
            });
